# Real-Time Data Streaming Pipeline  
### Airflow + Kafka + Spark Structured Streaming + Cassandra (Dockerized)

---

##  Overview

This project implements a fully containerized real-time data streaming pipeline using:

- **Apache Airflow** ‚Äì workflow orchestration  
- **Apache Kafka (Confluent stack)** ‚Äì event streaming backbone  
- **Apache Spark Structured Streaming** ‚Äì real-time processing engine  
- **Apache Cassandra** ‚Äì distributed NoSQL storage  
- **Docker Compose** ‚Äì local infrastructure orchestration  

The pipeline simulates a production-style streaming architecture by ingesting live user data from an external API, streaming it through Kafka, processing it with Spark, and persisting it into Cassandra.

---

## System Architecture

![System Architecture](system_architecture.png)

### Data Flow

```
Random User API ‚Üí Airflow Producer ‚Üí Kafka Topic ‚Üí Spark Streaming ‚Üí Cassandra
```

1. Airflow triggers a short-lived producer job (demo-style ingestion).
2. Producer publishes JSON events to Kafka (`users_created` topic).
3. Spark consumes and parses the stream.
4. Processed records are written to Cassandra.

---

## üß† Why This Architecture?

This project demonstrates core distributed data engineering principles:

| Component | Role | Why It‚Äôs Used |
|------------|-------|---------------|
| Airflow | Orchestration | Manages reproducible, schedulable ingestion workflows |
| Kafka | Event Streaming | Durable, scalable message buffer between producer and consumer |
| Spark Structured Streaming | Processing | Fault-tolerant, scalable stream computation engine |
| Cassandra | Storage | Distributed write-optimized NoSQL database |
| Docker | Infrastructure | Portable local deployment |

This decoupled architecture improves:

- Scalability  
- Fault isolation  
- Reproducibility  
- Horizontal processing capability  

---

## üõ† Tech Stack

- Python 3.9+
- Apache Spark 3.5.x
- Kafka (Confluent Platform)
- Cassandra 4.x
- Apache Airflow 2.x
- Docker & Docker Compose

---

## üìÇ Project Structure

```
Airflow-Kafka-Spark-Cassandra/
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ kafka_stream.py        # Airflow DAG (Kafka producer)
‚îÇ
‚îú‚îÄ‚îÄ spark_stream.py            # Spark Structured Streaming consumer
‚îú‚îÄ‚îÄ docker-compose.yml         # Infrastructure definition
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ system_architecture.png    # Architecture diagram
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Setup & Execution

### 1Ô∏è‚É£ Start All Services

```bash
docker compose up -d
```

This launches:

- Airflow (Web UI)
- Kafka + Zookeeper
- Confluent Control Center
- Spark master + workers
- Cassandra
- PostgreSQL (Airflow metadata)

---

### 2Ô∏è‚É£ Trigger Airflow DAG

- Open Airflow UI
- Enable the DAG
- Trigger manually

The DAG:
- Pulls user data from RandomUser API
- Publishes records to Kafka topic `users_created`

---

### 3Ô∏è‚É£ Start Spark Streaming Job

```bash
spark-submit   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0   spark_stream.py
```

Spark will:

- Subscribe to Kafka topic
- Parse JSON records
- Write streaming output to Cassandra

> **Note:** Make sure your Spark image/version matches the connector version you use above (e.g., Spark 3.5.x ‚Üî `:3.5.0`). If your stack uses Spark 3.4.x, update the package versions accordingly.

---

### 4Ô∏è‚É£ Verify Data in Cassandra

```bash
docker exec -it cassandra cqlsh
```

```sql
SELECT * FROM spark_streams.created_users;
```

---

## üß™ What This Project Demonstrates

- Real-time event ingestion
- Kafka topic configuration and streaming
- Spark Structured Streaming (micro-batch processing)
- Cassandra table design for streaming workloads
- Multi-container orchestration with Docker
- Debugging distributed service connectivity

---

## ‚ö† Engineering Tradeoffs & Notes

This is a local simulation of a distributed architecture.

In production, improvements would include:

- Dedicated long-running producer service (not Airflow loop)
- Schema enforcement via Schema Registry (Avro/Protobuf), not just raw JSON
- Stronger typing (e.g., consistent UUID handling end-to-end)
- Monitoring via Prometheus/Grafana
- Infrastructure-as-Code deployment (Terraform/K8s)
- Partition + throughput tuning for Kafka and Spark

Airflow is used here to **orchestrate** a demo ingestion workflow. In a production streaming system, ingestion services typically run independently of schedulers.

---

## üêõ Common Issues Encountered

<<<<<<< HEAD
- Configured Kafka dual-listener setup (`PLAINTEXT` for internal Docker networking, `PLAINTEXT_HOST` for host access) to support both intra-container and host-machine connectivity
- Resolved Python 3.12+ incompatibility with `cassandra-driver` by running Spark jobs inside the container (Python 3.8 environment)
- Managed Spark permission issues on the `apache/spark` image by running `spark-submit` and `pip install` as root
- Debugged a `NullKeyColumnException` caused by missing UUID generation in the Kafka producer, which Cassandra requires as a primary key
=======
- Kafka listener configuration mismatches
- Cassandra driver compatibility with Python 3.12
- Docker memory limits affecting Spark
- Port binding conflicts

---

## üìà Future Improvements

- Add Spark checkpointing for fault tolerance
- Introduce watermarking & windowed aggregations
- Add structured logging
- Implement exactly-once write semantics (where applicable)
- Deploy via Kubernetes

---

## üéØ Learning Outcome

This project reinforced:

- Distributed systems fundamentals  
- Streaming vs batch processing differences  
- Decoupled microservice-style architectures  
- Operational debugging across containerized services  
>>>>>>> 4b8cf0a (Updated README)
